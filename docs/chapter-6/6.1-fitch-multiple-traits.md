




## Learning objectives
In this lesson you will:

- Learn methods for filtering a two dimensional character array with `numpy`.
- Write a function to calculate Fitch parsimony score for multiple traits.
- 


## Character matrix

Most phylogenetic datasets will contain measurements of many traits, and 
so let's now expand our Fitch parsimony algorithm to accomodate such data, 
where the returned score will represent the sum of events across each measured 
trait. A collection of multiple trait measurements is typically stored in a
2-dimensional *matrix* (also termed an *array*). An example character matrix 
is shown below, and in Python would typically be stored either as 
a `numpy.ndarray`, or as a `pandas.DataFrame`. 

This example includes seven binary characters measured across five taxa. 
Another common type of data matrix is a sequence alignment, which can often 
span many thousands of columns, where each can take one of four possible 
states (A, C, G, T), or sometimes even ambiguous states (recorded as IUPAC 
character R, S, Y, W, M). Typically, the rows of a character matrix will
represent the taxa, and the columns are each a different trait; however,
this is not always the case, and so it is useful to examine your data and
check for row and column labels.

<!-- A data matrix 
could also represent many traits with different numbers of character states, 
such as with morphological measurements, in which case it can be important to 
also record the number of allowable states for each trait. -->
In this lesson, because we are still focused on the topic of parsimony tree 
inference, we will focus only on discrete characters, and not on quantitative 
character matrices. Ideally, the Fitch algorithm that we design below should 
work properly to return a score for any discrete character matrix. Let's 
start with an example using a tree and the binary character matrix below.

```py
import pandas as pd
import numpy as np
import toytree
```

```py
# example five tip tree (with tip names alpha-epsilon)
tree = toytree.tree("(((alpha,delta),gamma),(beta,epsilon));")

# example character array
arr = np.array([
    [1, 0, 0, 1, 1 ,0, 0],
    [0, 0, 1, 0, 0, 0, 0],
    [1, 1, 0, 0, 0, 0, 0],
    [1, 1, 0, 1, 1, 1, 0],
    [0, 0, 1, 1, 1, 0, 0],
])
```

## Data Organization
In the last notebook we used the `ToyTree.set_node_data` function to set 
character data as attributes of Nodes of a tree, which provided an easy 
way to keep data correctly associated with the proper taxon names. In the 
example above, you can see that the data matrix stored as a `numpy.array` 
is less clear, and could be interpreted ambiguously about which rows 
correspond to which taxon names. To reduce errors, it is good practice to 
retain clarity about the assignment of data to tips in a tree, since the 
order of tips is changeable, and often arbitrary. 

One way to more clearly label an array is to wrap it as a `pandas.DataFrame`
object, like below. This has similar functionality to a `numpy.ndarray`, and
in fact stores the data as an array under the hood, but it also includes
labels on the rows and columns. These labels can be used to index items
from the dataframe by row (index) and column names, as well as by indexing
with numeric indices, like in `numpy`. `pandas` has several convenient 
functions for reading and writing data from CSV and other tabular formatted 
data files, and thus is commonly used for keeping data organized and 
displaying it.

```py
# generate an example character matrix as a pandas dataframe
data = pd.DataFrame(
    index=["alpha", "beta", "gamma", "delta", "epsilon"],
    data=arr,
)

# display the dataframe
data
```
<figure markdown>
|         |    0 |    1 |    2 |    3 |    4 |    5 |  6  |
|:--------|-----:|-----:|-----:|-----:|-----:|-----:|----:|
| alpha   |    1 |    0 |    0 |    1 |    1 |    0 |    0|
| beta    |    0 |    0 |    1 |    0 |    0 |    0 |    0|
| gamma   |    1 |    1 |    0 |    0 |    0 |    0 |    0|
| delta   |    1 |    1 |    0 |    1 |    1 |    1 |    0|
| epsilon |    0 |    0 |    1 |    1 |    1 |    0 |    0|
</figure>

==add plot of data on the tips of tree here, demonstrating indexing from DF.==

## Array Operations

Although dataframes are great for clarity and organization, they are 
considerably slower than working with `numpy` arrays, and so when writing
efficient code I tend to extract the array from a dataframe and operate on
it directly. The example below is a somewhat complex function, named 
`preprocess_matrix`, that we will use in this lesson to greatly speed up 
calculations of parsimony scores from a character matrix. 

The goal of this function is to reduce the total number of characters that 
actually need to be analyzed independently. In this respect, it performs
three operations: (1) we can skip any data columns that are invariant, such 
as "t6" in the table above, since we know that these will have zero events 
under parsimony; (2) Similarly, we can skip any columns that have only a 
single difference, such as "t5", since we know that regardless of the tree 
topology this will require only a 1 event under parsimony, and so we can 
simply count 1 for each of these; (3) Finally, we can reduce the amount of 
work by counting how many unique site patterns there are, and only applying 
our algorithm to these, and again multiplying the result by the number of 
times it occured. For example, columns "t3" and "t4" are identical, and 
similarly, "t0" and "t2" are the same, but simply inverted. 

Tricks like this are common in phylogenetic algorithms, especially
when dealing with very large character matrices. Reducing the number of 
unique characters that need to be independently analyzed will have a large
effect when the algorithm must be repeated thousands of times, as is common
when performing a full tree inference algorithm. The full `preprocess_matrix`
function can be found below. We will go through it step by step to describe 
each operation, with reference to the line numbers of code blocks.

??? Example "preprocess_matrix function definition"
    ```py linenums="1"
    from typing import Tuple

    def preprocess_matrix(data: pd.DataFrame) -> Tuple[int, np.ndarray, pd.DataFrame]:
        """Return a score and dict of sites to be processed.
        
        This func will remove invariant sites and sites with only a single
        autapomorphy to get a reduced DataFrame with only unique patterns. 

        Parameters
        ----------
        data: pandas.DataFrame
            A pandas dataframe character matrix with taxon names as index.

        Returns
        -------
        Tuple 
            A 3-part tuple with (score, counts, data). The score is an int 
            for the number of singleton sites; the counts is an array of ints
            for the number of occurrence of each unique site pattern; the data
            is a DataFrame with the unique site patterns.
        """
        # get data as a numpy array
        arr = np.array(data)

        # get int array counting how many samples match the state of 
        # the first sample in each column. We will use this for filtering.
        counts = np.sum(arr == arr[0], axis=0)
        
        # get boolean array of invariant columns, i.e., all items match the first.
        invariant = counts == arr.shape[0]
        
        # get boolean array of singleton columns, i.e., nmaching is 1 or N-1
        singletons = (counts == 1) | (counts == arr.shape[0] - 1)
        
        # combine boolean masks and filter columns from data
        arr = arr[:, np.invert(invariant | singletons)]

        # count the number of singleton sites
        score = singletons.sum()

        # get unique site patterns and their counts
        arr, counts = np.unique(arr, axis=1, return_counts=True)

        # find columns that are the same PATTERN as others, i.e., recode patterns
        # such that [0, 1, 1, 2] == [1, 2, 2, 0] == [2, 0, 0, 1], etc.
        uniq_dict = {}
        for cidx in range(arr.shape[1]):
            
            # get a column of data. e.g., [1, 1, 0, 1, 2]
            col = arr[:, cidx]
            
            # get index of first occurrence of each unique base, e.g., [0, 2, 4],
            _, idxs = np.unique(col, return_index=True)
            
            # get order of indices, e.g., [0, 1, 2]
            order = np.argsort(idxs).tolist()
            
            # get site as ordered index of unique patterns, e.g., [0, 0, 1, 0, 2]
            site = tuple((order.index(i) for i in col))
            
            # record each uniq pattern, or add to it if already mirrored.
            if site not in uniq_dict:
                uniq_dict[site] = counts[cidx]
            else:
                uniq_dict[site] += counts[cidx]
        
        # convert unique patterns and counts back into a dataframe and array
        arr = np.column_stack(list(uniq_dict))
        data = pd.DataFrame(data=arr, index=data.index)
        counts = np.array(list(uniq_dict.values()))

        # return the score, counts, and data
        return score, counts, data
    ```

As an example, when we apply this function to our data it is reduced from 
7 sites to only 3 unique sites, two of which occurred twice, and one that
occurred once. In addition, the parsimony score for this matrix is already
1, since there was one site that had a pattern with only one sample different
from the others. 

#### Pandas vs Numpy speed
This preprocessing function will only need to be run once, at the beginning
of a tree inference program, and therefore its overall speed is not of 
great concern. However, in many other functions that we write the speed of
the code will be important, since the function may need to be iterated many
thousands of times, such as a function to calculate the fitch parsimony 
score. Thus it is useful to learn some tricks for writing more efficient 
Python code.

One trick that I mentioned concerns the use of `pandas` versus `numpy`. A
quick demonstration can show clearly that `numpy` is much faster, even for
very simple operations like 2-dimensional indexing. Let's look a quick example, 
using our handy `%%timeit` magic command. 

```py
# create a numpy array of all zeros of size (100 x 100)
arr = np.zeros(shape=(100, 100), dtype=int)

# create a dataframe containing the same data
data = pd.DataFrame(data=arr)
```

First let's measure the time to index a single element using `numpy`:
```py
%%timeit
arr[50, 50]
```
```py
# 90.7 ns ± 2.88 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)
```
And next let's index a single element using `pandas`:
```py
%%timeit
data.iloc[50, 50]
```
```py
# 20.1 µs ± 447 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
```
Indexing with `numpy` is *>200X faster!* Thus, if your function involves 
indexing or performing almost any operation over a dataframe many times, it
almost always beneficial to convert it to a numpy array first. 

For example, in our preprocessing function, we convert the dataframe to an 
array at the beginning on line 23...

```py linenums="22"
    # get data as a numpy array
    arr = np.array(data)
```

... then we perform operations of the array, before finally converting it
back to a dataframe at the end, on line 69, so that the data is returned 
to the user still containing nice labels.

```py linenums="69"
    data = pd.DataFrame(data=arr, index=data.index)
```


#### Boolean masking
One of the fastest and most useful ways to filter data in `numpy` arrays
is by using *boolean masking*. This involves generating an array full
of `boolean` values and using it to index the original array. This will 
return the values of the array at all positions where the boolean
mask is `#!Python True`, and not return where it is `#!Python False`. This
can be applied to individual cells of an array, but even more useful is
to apply it to columns or rows of an array. Let's walk through the 
example below:

```py linenums="24"
    # get int array counting how many samples match the state of 
    # the first sample in each column. We will use this for filtering.
    counts = np.sum(arr == arr[0], axis=0)
```

This one line is actually doing something fairly complex, so let's look in
more detail. Below you can see the value of the original `arr`, as well 
as of the value represented by the operation *inside* of the function, 
`arr == arr[0]`. This statement, which uses `==` operator, is asking "are
these things equal?". Because this statement is applied to a `numpy` array, 
it is *broadcast* to apply to each item. In this case, that involves asking 
for each item, whether it matches the corresponding item in the first row. 

A bit tricky, right? But this turns out to be a very useful and common 
operation when working with arrays of this type. The result of this statement
is shown below as a variable named `mask`. You can see that it is a boolean 
array where the first row is all True, but other items may be True or False.
This function will work regardless of whether the data is binary or composed
of some other multistate characters.

The final statement below is the same as in our function. It takes the 
boolean array (which we named `mask`) and calculates the sum over `axis=0`, 
which means to get the sum of each column. Our final result is an integer 
array with the number of values matching the first row item. As we'll see, 
these counts are useful for applying our next filters to ask whether a site
is invariant, or if it contains only one difference.

```py
# view the original data arr
print(f"arr:\n{arr}")

# compare arr columns-wise to the value in the first row of arr
mask = arr == arr[0]
print(f"\nmask:\n{mask}")

# get sum of True values in each column of the boolean mask
counts = np.sum(arr == arr[0], axis=0)
print(f"\ncounts:\n{counts}")
```
```py
# arr:
# [[1 0 0 1 1 0 0]
#  [0 0 1 0 0 0 0]
#  [1 1 0 0 0 0 0]
#  [1 1 0 1 1 1 0]
#  [0 0 1 1 1 0 0]]

# mask:
# [[ True  True  True  True  True  True  True]
#  [False  True False False False  True  True]
#  [ True False  True False False  True  True]
#  [ True False  True  True  True False  True]
#  [False  True False  True  True  True  True]]

# counts:
# [3 3 3 3 3 4 5]
```

==TODO==

```py linenums="28"
    # get boolean array of invariant columns, i.e., all items match the first.
    invariant = counts == arr.shape[0]

    # get boolean array of singleton columns, i.e., nmaching is 1 or N-1
    singletons = (counts == 1) | (counts == arr.shape[0] - 1)

    # combine boolean masks and filter columns from data
    arr = arr[:, np.invert(invariant | singletons)]
```

#### Array indexing
==TODO==


#### Unique patterns
==TODO==


```py
# example to reduce character matrix to unique patterns
score, counts, reduced_data = preprocess_matrix(data)
print(score)
print(counts)
print(reduced_data)
```

```py
# 1
# [1 2 2]
#          0  1  2
# alpha    0  0  0
# beta     0  1  1
# gamma    1  0  1
# delta    1  0  0
# epsilon  0  1  0
```


## Fitch Parsimony Function
Now we can compute the rest of the parsimony score by 
analyzing these remaining unique patterns on the tree. For this, we can 
use a modification of our function from the last page which includes our
`preprocess_matrix` function within it.

```py
def fitch_parsimony(tree: toytree.ToyTree, data: pd.DataFrame) -> int:
    """Return Fitch parsimony score given a tree and single trait.

    Parameters
    ----------
    tree: ToyTree
        A tree on which to count state changes.
    data: DataFrame
        A DataFrame with tree tip labels in the index and >=1 columns
        of discrete character data as int or str.
        
    Returns
    -------
    int
        The minimum changes required for trait data to evolve on this tree.
    """
    # reduce matrix to unique sites and counts, and get initial score
    score, counts, data = preprocess_matrix(data)

    # sort data to tip idxorder and convert to array (for faster indexing)
    data = data.loc[tree.get_tip_labels()].values

    # iterate over each column index of data
    for cidx in range(data.shape[1]):

        # iterate over Nodes in idxorder (postorder sorted) traversal
        for node in tree:

            # leaves are visited first, and converted to a set type
            if node.is_leaf():
                char = data[node.idx, cidx]
                node._fitch = set([char])

            # internal Nodes examine the sets of their children's states
            if not node.is_leaf():

                # check for shared (intersecting) states
                shared = set.intersection(*(i._fitch for i in node.children))

                # if any states are shared then ancestor inherits this state
                if shared:
                    node._fitch = shared

                # if none shared, then store the union and increment counter
                else:
                    node._fitch = set.union(*(i._fitch for i in node.children))
                    score += counts[cidx]
    return score
```

An implementation on our example dataset:
```py
fitch_parsimony(tree, data)
```

```py
# 5
```

<!-- 
If we wanted to calculate additional information about our character 
matrix we might return the parsimony score for each trait individually, 
perhaps as an array, instead of just returning the sum, so that we could
identify which characters are good or poor fits to a particular tree.

...
-->