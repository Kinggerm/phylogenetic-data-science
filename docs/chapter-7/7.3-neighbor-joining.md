



# Neighbor-Joining Tree Inference


## Introduction
Neighbor-joining (NJ) is an algorithm for inferring trees from distance
data, first described by Saitou and Nei (1987). In contrast to UPGMA it 
does not assume a clock, and is therefore is less likely to infer the 
wrong tree due to rate variation. In fact, the NJ method is guaranteed to 
recover the true tree if the distance matrix is an exact reflection of 
distances on the true tree. 

For very large data sets, even when employing bootstrapping to re-sample
a data matrix and infer multiple trees, the greedy algorithmic approach to
building using clustering results in only a very tiny proportion of tree 
space being explored.


## Example Data
To demonstrate the neighbor-joining algorithm we will re-use the same 
dataset as in the previous lessons, which is a distance matrix from 
Sarich (1969), and which was similarly used as an example in Felsenstein
(2004). 

```py
NAMES = ["dog", "bear", "raccoon", "weasel", "seal", "sea lion", "cat", "monkey"]
DATA = pd.DataFrame(
    index=NAMES, columns=NAMES,
    data=np.array([
        [0, 32, 48, 51, 50, 48, 98, 148],
        [32, 0, 26, 34, 29, 33, 84, 136],
        [48, 26, 0, 42, 44, 44, 92, 152],
        [51, 34, 42, 0, 44, 38, 86, 142],
        [50, 29, 44, 44, 0, 24, 89, 142],
        [48, 33, 44, 38, 24, 0, 90, 142],
        [98, 84, 92, 86, 89, 90, 0, 148],
        [148, 136, 152, 142, 142, 142, 148, 0],
    ])
)
```

## Example Analysis
The code that we will develop below to implement a neighbor-joining tree
inference algorithm is available in the `toytree.infer` subpackage:

```py
# infer an NJ tree from a distance matrix and plot it.
TREE = toytree.infer.neighbor_joining(DATA)
TREE.draw();
```

==TODO: insert tree drawing here.==

## Algorithm
An algorithm for neighbor-joining is described by Studier and Keppler 1988,
starting from a distance matrix for N samples, where the distance between
sample $i$ and $j$ is $D_{ij}$. Below this algorithm is described both 
verbally, as well as in mathematical notation, and with Python code. To 
demonstrate how to translate these equations *into code*, we will start
first with how to interpret them, followed by writing psuedo-code to 
convert logic into Python, and finally, we will write a full Python
neighbor-joining algorithm.

### 1: get average distances
Compute the average distance ($u_i$) from each sample in the distance
matrix to the others (excluding a comparison of a sample to itself):

$$ u_i = \sum\limits_{j:j{\neq}i}^{n} \frac{D_{ij}}{(n-2)} $$

The stated goal of this step is to compute a value for each sample, so 
we know to expect the output to be a list or 1-D array of values. To 
compute this for each sample at a time we could perform a for-loop that
will visit each sample. So let's begin first by just writing the very first 
step of this algorithm, a for-loop to iterate over each sample. This could
be done with indexing, or, in this case where we have a DataFrame object,
we can simply iterate over the `.index` attribute (row names).

```py
# iterate over each sample in the data
for sample_i in data.index:
    # ... do whatever comes next.
    print(sample_i)
```

Next, we see a summation symbol ($\sum\limits_{j:j{\neq}i}^{n}$). Based
on the lower and upper bounds of the summation, this can be interpreted
to mean that we will sum *something* for each item $j$ from a collection
of items that ranges from $j$ to $n$. In *code-think*, this sounds like 
another for-loop, iterating over the samples once again, but now referring
to their index as $j$. It also says ${j:j{\neq}i}$, which means that we 
should skip any iteration where the $j$ is the same as $i$. In other words,
skip the iterations where a sample is compared to itself. 
Here's what we have now:

```py
# iterate over each sample in the data
for sample_i in data.index:
    for sample_j in data.index:
        if sample_i != sample_j:
            # ... do whatever comes next.
            print(sample_i, sample_j)
```

Finally, we are to the part of the equation that defines the measurement
that awe are summing over: $\frac{D_{ij}}{(n-2)}$. At this point in the code,
we have both $i$ and $j$, which we can use to index a value from the 
matrix $D$. And we know that $n$ is the number of samples, which is the same
as the number of columns in the matrix. So let's calculate the $u_i$ now
by putting this all together. Here I use a dictionary to store values that
will be summed during the iteration.

```py
# a dict to store u_i values
u_values = {}

# get number of samples
nsamples = data.shape[1]

# iterate over samples
for sample_i in data.index:
    # start u_i value at 0
    u_values[sample_i] = 0
    for sample_j in data.index:
        if sample_i != sample_j:
            # add this value to summed u_i
            u_values[sample_i] += data.loc[sample_i, sample_j] / (nsamples - 2)
print(u_values)
```
```py
# {'dog': 79.16666666666667,
#  'bear': 62.33333333333333,
#  'raccoon': 74.66666666666667,
#  'weasel': 72.83333333333334,
#  'seal': 70.33333333333333,
#  'sea lion': 69.83333333333333,
#  'cat': 114.5,
#  'monkey': 168.33333333333334}
```

The example above was written to be easy to read and understand. But, 
we can actually compute the same result much faster, and more concisely,
using broadcasting with `numpy` arrays (or with `pandas`). 
For example, we can compute the sum of distance values across each row of 
the distance matrix by calling `.sum(axis=0)`, and then divide all of the 
row sums by the same constant to get the result as an array. We will use 
this notation in our final function.

```py
arr = np.array(data)
uvals = arr.sum(axis=0) / (arr.shape[0] - 2)
print(uvals)
```
```py
# array([ 79.16666667,  62.33333333,  74.66666667,  72.83333333,
#         70.33333333,  69.83333333, 114.5       , 168.33333333])
```

### 2: find smallest distance
We now have our 2-D distance matrix $D_{ij}$, and a 1-D array of 
$u_{i}$ values. In step 2, we need to find the smallest value of
$D_{ij} - u_i - j_i$, which represents a modified distance
that is corrected for neighbor-ness. The indices of the smallest
values will represent the two samples that will be clustered 
in this iteration of the algorithm. 

In the code below I subtract the $u_i$ values (`uvals`) from each column of 
the distance matrix (`arr`) using numpy broadcasting, and also subtract
`uvals` from each row of `arr` with broadcasting by first expanding the 
dimension of `uvals` so that its shape is `(n, 1)`. This is a 
simple trick in numpy to broadcast row-wise versus column-wise. The new
distance values are stored as `c_arr`. The following code block creates
a mask to select only the values not on the diagonal, and then finds the
index of the minimum value.

```py
# subtract neighbor distances column-wise and row-wise from D
c_arr = arr - uvals - np.expand_dims(uvals, 1)

# find index of min value not on the matrix diagonal
mask = np.ones(c_arr.shape, dtype=bool)
np.fill_diagonal(mask, False)
idxs, jdxs = np.where(c_arr == c_arr[mask].min())

# get first index value in case there is a tie
i = idxs[0]
j = jdxs[0]
```

To further demonstrate, you can see below what `uvals` looks like before 
and after adding a dimension, and how it is subtracted from each column
or row of the distance array. (To make these arrays easier to read by 
showing only two floating point values I called 
`np.set_printoptions(precision=2)`).


The values in the 2-D distance array `arr`:
```py
print(arr)
```
```py
# [[  0  32  48  51  50  48  98 148]
#  [ 32   0  26  34  29  33  84 136]
#  [ 48  26   0  42  44  44  92 152]
#  [ 51  34  42   0  44  38  86 142]
#  [ 50  29  44  44   0  24  89 142]
#  [ 48  33  44  38  24   0  90 142]
#  [ 98  84  92  86  89  90   0 148]
#  [148 136 152 142 142 142 148   0]]
```

Subtract the $i^{th}$ $u_i$ value from all items in the $i^{th}$ **column** of `arr`:
```py
print(uvals)
```
```py
# [ 79.17  62.33  74.67  72.83  70.33  69.83 114.5  168.33]
```
```py
# subtract column-wise
print((data - uvals)
```
```py
# [[ -79.17  -30.33  -26.67  -21.83  -20.33  -21.83  -16.5   -20.33]
#  [ -47.17  -62.33  -48.67  -38.83  -41.33  -36.83  -30.5   -32.33]
#  [ -31.17  -36.33  -74.67  -30.83  -26.33  -25.83  -22.5   -16.33]
#  [ -28.17  -28.33  -32.67  -72.83  -26.33  -31.83  -28.5   -26.33]
#  [ -29.17  -33.33  -30.67  -28.83  -70.33  -45.83  -25.5   -26.33]
#  [ -31.17  -29.33  -30.67  -34.83  -46.33  -69.83  -24.5   -26.33]
#  [  18.83   21.67   17.33   13.17   18.67   20.17 -114.5   -20.33]
#  [  68.83   73.67   77.33   69.17   71.67   72.17   33.5  -168.33]]
```

Alternatively, subtract the $i^{th}$ $u_i$ value from all items in the 
$i^{th}$ **row** of `arr`.
```py
# reshape to (n,1) to align row-wise with arr
print(np.expand_dims(uvals, 1))
```
```py
# [[ 79.17]
#  [ 62.33]
#  [ 74.67]
#  [ 72.83]
#  [ 70.33]
#  [ 69.83]
#  [114.5 ]
#  [168.33]]
```

```py
# subtract row-wise
print((data - np.expand_dims(uvals, 1)))
```
```py
# [[ -79.17  -47.17  -31.17  -28.17  -29.17  -31.17   18.83   68.83]
#  [ -30.33  -62.33  -36.33  -28.33  -33.33  -29.33   21.67   73.67]
#  [ -26.67  -48.67  -74.67  -32.67  -30.67  -30.67   17.33   77.33]
#  [ -21.83  -38.83  -30.83  -72.83  -28.83  -34.83   13.17   69.17]
#  [ -20.33  -41.33  -26.33  -26.33  -70.33  -46.33   18.67   71.67]
#  [ -21.83  -36.83  -25.83  -31.83  -45.83  -69.83   20.17   72.17]
#  [ -16.5   -30.5   -22.5   -28.5   -25.5   -24.5  -114.5    33.5 ]
#  [ -20.33  -32.33  -16.33  -26.33  -26.33  -26.33  -20.33 -168.33]]
```

The final result was to perform both subtractions on the distance
matrix and then to find the index of the lowest off-diagonal value.
In the matrix below the lowest value is -134.83, at index `(6, 7)`, 
or `(7, 6)`, depending on which side of the matrix is searched first.

```py
print(arr - uvals - np.expand_dims(uvals, 1))
```
```py
# [[-158.33 -109.5  -105.83 -101.    -99.5  -101.    -95.67  -99.5 ]
#  [-109.5  -124.67 -111.   -101.17 -103.67  -99.17  -92.83  -94.67]
#  [-105.83 -111.   -149.33 -105.5  -101.   -100.5   -97.17  -91.  ]
#  [-101.   -101.17 -105.5  -145.67  -99.17 -104.67 -101.33  -99.17]
#  [ -99.5  -103.67 -101.    -99.17 -140.67 -116.17  -95.83  -96.67]
#  [-101.    -99.17 -100.5  -104.67 -116.17 -139.67  -94.33  -96.17]
#  [ -95.67  -92.83  -97.17 -101.33  -95.83  -94.33 -229.   -134.83]
#  [ -99.5   -94.67  -91.    -99.17  -96.67  -96.17 -134.83 -336.67]]
```



### 3: get branch lengths
Given that we just found the $i$ and $j$ indices above, the code to calculate
the branch lengths of these Nodes to their internal Node parent 
is very simple:

$$ 
v_i = \frac{1}{2}D_{ij} + \frac{1}{2}(u_i - u_j) 
$$

$$
v_j = \frac{1}{2}D_{ij} + \frac{1}{2}(u_j - u_i) 
$$

```py
# get branch lengths for Nodes i and j
v_i = 0.5 * arr[i, j] + 0.5 * (uvals[i] - uvals[j])
v_j = 0.5 * arr[i, j] + 0.5 * (uvals[j] - uvals[i])
```

This information could be used right away to begin to construct a tree, 
for example by creating `toytree.Node` instances and connecting them; or,
this information could be stored until the end, after all iterations of 
the algorithm are finished, and the final tree constructed then. Either
way, the information that exists at this point in the algorithm, during
our first iteration, is that two Nodes have now been joined, and the
other tips remain not yet joined. Before we can proceed to the next 
iteration, we need to update the distance matrix to remove the two tip 
Nodes that are not longer relevant, and to replace them with their new
parent Node, and its distances to the other Nodes.

==TODO: insert drawing here...==

### 4: update matrix distances
We now need to update the distance matrix so that ...

```py
# get dimension of the new matrix and make a new empty array
new_dim = arr.shape[0] - 1
new_arr = np.zeros(shape=(new_dim, new_dim))

# create a boolean mask of old matrix size, with False at i and j.
mask = np.ones(arr.shape[0], dtype=bool)
mask[[i, j]] = False

# fill new matrix with dists from old matrix for non-i,j samples
new_arr[:new_dim - 1, :][:, :new_dim - 1] = arr[mask, :][:, mask]

# get distances from each remaining sample to the new ancestor of i,j
new_arr[-1, :-1] = new_arr[:-1, -1] = (arr[i] + arr[j] - arr[i, j])[mask] / 2.

# replace arr with the new arr
arr = new_arr
```

### 5: repeat if >2 samples remain


## NEXT...



